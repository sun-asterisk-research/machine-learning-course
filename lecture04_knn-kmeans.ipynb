{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4: KNN & K-Means\n",
    "Again, fill the ellipses `...` with code, and don't remove `assert` lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use the Iris dataset again.\n",
    "Just goes to show that `sklearn` makes things way too easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our dataset\n",
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "X, Y = data['data'], data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split our data into training and testing set with 90:10 ratio\n",
    "# use a fixed random state for reproducible results\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z-score normalization.\n",
    "# Remember to scale the training and test set separately to avoid data snooping.\n",
    "# We use the training set's mu and sigma for the test set.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN: $k$-Nearest Neighbors\n",
    "Evaluate the test set with data from the training set.\n",
    "\n",
    "In case of ties, pick the smallest class (i.e. we prefer class 0 to class 1 to class 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "import operator\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def hakaru_distance(input1, input2):\n",
    "    # input1 cố định: input2^2 - 2*input1*input2\n",
    "    res = np.array(input2).dot(input2) - 2*np.array(input1).dot(input2)\n",
    "    return res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def weight(distance):\n",
    "    res = np.exp(-distance)\n",
    "    return res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember, no training is needed for KNN!\n",
    "def evaluateKNN_single(k, x_train, y_train, data):\n",
    "    '''\n",
    "    Evaluate the classification for `data` with k-nearest neighbor\n",
    "    given training set (x_train, y_train).\n",
    "\n",
    "    Note that this function takes in one input instead of the whole\n",
    "    testing set.\n",
    "\n",
    "    Input:\n",
    "        k      : hyperparameter for KNN\n",
    "        x_train: features of training set\n",
    "        y_train: labels of training set\n",
    "        data   : features of the data point to be evaluated\n",
    "    Output:\n",
    "        Classification of the input data point.\n",
    "    '''\n",
    "    \n",
    "    # IMPLEMENT HERE\n",
    "    distance = []\n",
    "    for i in range(len(x_train)):\n",
    "        distance.append([hakaru_distance(data, x_train[i]), y_train[i]])\n",
    "    distance.sort(key=lambda x:x[0])\n",
    "    distance = distance[:k]\n",
    "\n",
    "    mydict = {}\n",
    "    dis_1 = np.array(data).dot(data)\n",
    "    for i in range(k):\n",
    "        if distance[i][1] not in mydict:\n",
    "            mydict[distance[i][1]] = 0\n",
    "        mydict[distance[i][1]] += weight(distance[i][0] + dis_1)\n",
    "    return max(mydict, key=mydict.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation code for the whole dataset\n",
    "def evaluateKNN(k, x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test):\n",
    "    correct = sum(map(lambda x: evaluateKNN_single(k, x_train, y_train, x[0]) == x[1], zip(x_test, y_test)))\n",
    "    print(f'Test accuracy with k={k}: {correct/len(y_test)*100:.4f}% ({correct}/{len(y_test)})')\n",
    "    # return the number of correct evaluations for us to check with the solution\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with k=5: 100.0000% (15/15)\n"
     ]
    }
   ],
   "source": [
    "# and let's see how good is our model with k=5\n",
    "assert evaluateKNN(5) == len(y_test), \"Incorrect accuracy for 5-NN!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with k=1: 93.3333% (14/15)\n"
     ]
    }
   ],
   "source": [
    "# What if we use 1-NN?\n",
    "assert evaluateKNN(1) == len(y_test) - 1, \"Incorrect accuracy for 1-NN!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means clustering\n",
    "Use the first 3 data points as initial cluster centroids (medoids anyone?)\n",
    "\n",
    "Run the recaliberation step 10 times. Yes, it converges that quickly for a NP-hard problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_classification(x_data, centroids):\n",
    "    '''\n",
    "    A helper function that you will need later.\n",
    "    Classifies the points to their nearest cluster.\n",
    "    \n",
    "    Input:\n",
    "        x_data   : the data points\n",
    "        centroids: the cluster centroids\n",
    "    Output:\n",
    "        The centroid numbers that each data point belongs to (i.e. is nearest)\n",
    "    '''\n",
    "    \n",
    "    # IMPLEMENT HERE\n",
    "    distance = []\n",
    "    for i in range(len(centroids)):\n",
    "        distance.append([hakaru_distance(x_data, centroids[i]), i])\n",
    "\n",
    "    distance.sort(key=lambda x:x[0])\n",
    "    return distance[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import random"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(x_train, k, step):\n",
    "    '''\n",
    "    An implementation of K-means clustering.\n",
    "    \n",
    "    Input:\n",
    "        k      : number of clusters\n",
    "        x_train: training dataset\n",
    "        step   : number of recaliberation steps\n",
    "    Output:\n",
    "        The centroids of the clusters (a k x d matrix)\n",
    "    '''\n",
    "    \n",
    "    # IMPLEMENT HERE\n",
    "\n",
    "    centroids = np.array(x_train[:3])\n",
    "\n",
    "    for i in range(step):\n",
    "        my_list = []\n",
    "\n",
    "        for j in range(k):\n",
    "            my_list.append([])\n",
    "\n",
    "        for j in x_train:\n",
    "            my_list[get_cluster_classification(j, centroids)].append(j)\n",
    "\n",
    "        for j in range(k):\n",
    "            centroids[j] = sum(my_list[j])/len(my_list[j])\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we know that there are three classes\n",
    "centroids = kmeans(x_train, k=3, step=10)\n",
    "assert np.allclose(centroids, np.array([\n",
    "    [-1.02028733,  0.90854287, -1.32521428, -1.27540932],\n",
    "    [ 0.99363929,  0.01896468,  0.90355632,  0.92076921],\n",
    "    [-0.22539812, -1.02749927,  0.23322382,  0.15491878]\n",
    "])), \"Incorrect centroids for K-means!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means++\n",
    "Use the 4th data point as the intial centroid each step ([chosen with randomness](https://xkcd.com/221/)):\n",
    "- The first initial centroid should be the 4th data point.\n",
    "- The next initial centroids should be the 4th furthest data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeanspp(x_train, k, step):\n",
    "    '''\n",
    "    An implementation of K-means++ clustering.\n",
    "    \n",
    "    Input:\n",
    "        k      : number of clusters\n",
    "        x_train: training dataset\n",
    "        step   : number of recaliberation steps\n",
    "    Output:\n",
    "        The centroids of the clusters (a k x d matrix)\n",
    "    '''\n",
    "    # initialize the centroids according to the above criteria\n",
    "    \n",
    "    # IMPLEMENT HERE\n",
    "    centroids = []\n",
    "    for i in range(1, k+1):\n",
    "        centroids.append(x_train[i*4])\n",
    "    centroids = np.array(centroids)\n",
    "    # the rest should be identical to kmeans()\n",
    "    \n",
    "    # IMPLEMENT HERE\n",
    "\n",
    "\n",
    "    for i in range(step):\n",
    "        my_list = []\n",
    "\n",
    "        for j in range(k):\n",
    "            my_list.append([])\n",
    "\n",
    "        for j in x_train:\n",
    "            my_list[get_cluster_classification(j, centroids)].append(j)\n",
    "\n",
    "        for j in range(k):\n",
    "            centroids[j] = sum(my_list[j])/len(my_list[j])\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now check if you did it correctly.\n",
    "centroidspp = kmeanspp(x_train, k=3, step=10)\n",
    "assert np.allclose(centroidspp, np.array([\n",
    "    [-0.0118057 , -0.87997489,  0.36942197,  0.30573876],\n",
    "    [-1.03358934,  0.84835232, -1.32732076, -1.27380566],\n",
    "    [ 1.15200055,  0.18878042,  0.98903982,  1.01136932]\n",
    "\n",
    "])), \"Incorrect centroids for K-means++!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Classification using clustering\n",
    "We can treat each cluster to be of a different class, and the class with most points in each cluster is the classification for that cluster. Think voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the associated classification for each cluster\n",
    "def get_cluster_label(centroids, x_train, y_train):\n",
    "    '''\n",
    "    Get the classification for each cluster using training set.\n",
    "    \n",
    "    Input:\n",
    "        centroids: the centroids of the clusters\n",
    "        x_train  : features of training set\n",
    "        y_train  : labels of training set\n",
    "    Output:\n",
    "        The classifications for the clusters.\n",
    "    '''\n",
    "    # remember to return a numpy array instead of a Python list!\n",
    "    \n",
    "    # IMPLEMENT HERE\n",
    "    my_list = []\n",
    "\n",
    "    for i in range(len(centroids)):\n",
    "        my_list.append({})\n",
    "        for j in range(max(y_train) + 1):\n",
    "            my_list[i][j] = 0\n",
    "\n",
    "    for i in range(len(x_train)):\n",
    "        my_list[get_cluster_classification(x_train[i], centroids)][y_train[i]] += 1\n",
    "\n",
    "    res = []\n",
    "    print(my_list)\n",
    "    for i in my_list:\n",
    "        res.append(max(i, key=i.get))\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{0: 43, 1: 0, 2: 0}, {0: 0, 1: 14, 2: 39}, {0: 1, 1: 30, 2: 8}]\n",
      "[{0: 0, 1: 34, 2: 17}, {0: 44, 1: 0, 2: 0}, {0: 0, 1: 10, 2: 30}]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Incorrect K-means++ cluster label(s)!",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-67-499448e73ce5>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;31m# each cluster nicely belongs to a different class\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32massert\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mlabels\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mall\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"Incorrect K-means cluster label(s)!\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m \u001B[1;32massert\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mlabelspp\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mall\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"Incorrect K-means++ cluster label(s)!\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAssertionError\u001B[0m: Incorrect K-means++ cluster label(s)!"
     ]
    }
   ],
   "source": [
    "labels = get_cluster_label(centroids, x_train, y_train)\n",
    "labelspp = get_cluster_label(centroidspp, x_train, y_train)\n",
    "# each cluster nicely belongs to a different class\n",
    "assert (labels == [0, 2, 1]).all(), \"Incorrect K-means cluster label(s)!\"\n",
    "assert (labelspp == [1, 2, 0]).all(), \"Incorrect K-means++ cluster label(s)!\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_kmeans_classification(centroids, labels, x_data):\n",
    "    '''\n",
    "    Get the classification for each data point using centroid labels.\n",
    "    \n",
    "    Input:\n",
    "        centroids: the centroids of the clusters\n",
    "        labels   : the labels for the clusters\n",
    "        x_data   : the data to be classified\n",
    "    Output:\n",
    "        The classifications for the data.\n",
    "    '''\n",
    "    \n",
    "    # IMPLEMENT HERE\n",
    "    res = []\n",
    "\n",
    "    for i in x_data:\n",
    "        res.append(labels[get_cluster_classification(i, centroids)])\n",
    "\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the classifications\n",
    "y_train_pred = evaluate_kmeans_classification(centroids, labels, x_train)\n",
    "y_test_pred = evaluate_kmeans_classification(centroids, labels, x_test)\n",
    "y_train_pred_pp = evaluate_kmeans_classification(centroidspp, labelspp, x_train)\n",
    "y_test_pred_pp = evaluate_kmeans_classification(centroidspp, labelspp, x_test)\n",
    "\n",
    "# and check for correctness\n",
    "assert (y_train_pred == [2, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2, 0, 0, 1, 2, 0, 0, 0, 1, 2, 2, 0, 0, 1, 2, 2, 2, 2, 1, 2, 1, 0, 2, 1, 0, 0, 0, 1, 2, 0, 0, 0, 1, 0, 1, 2, 0, 1, 1, 0, 2, 1, 1, 1, 2, 1, 0, 1, 2, 0, 0, 1, 2, 0, 2, 0, 0, 2, 1, 2, 1, 1, 2, 1, 0, 0, 1, 2, 0, 0, 0, 1, 2, 0, 2, 2, 0, 1, 2, 2, 2, 2, 0, 2, 1, 2, 1, 1, 2, 1, 2, 1, 0, 1, 2, 2, 0, 1, 2, 2, 0, 2, 0, 2, 2, 2, 1, 2, 1, 2, 1, 2, 0, 1, 1, 0, 1, 2]).all()\n",
    "assert (y_test_pred == [1, 0, 2, 1, 2, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0]).all()\n",
    "assert (y_train_pred_pp == [2, 2, 1, 1, 2, 0, 1, 0, 2, 2, 2, 1, 2, 0, 0, 0, 0, 2, 0, 0, 1, 2, 0, 0, 0, 1, 2, 2, 0, 0, 1, 1, 2, 2, 2, 1, 2, 1, 0, 2, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 2, 0, 1, 1, 0, 1, 1, 1, 1, 2, 1, 0, 1, 1, 0, 0, 1, 2, 0, 1, 0, 0, 2, 1, 2, 1, 1, 2, 1, 0, 0, 1, 2, 0, 0, 0, 1, 2, 0, 2, 2, 0, 1, 1, 1, 2, 2, 0, 2, 1, 2, 1, 1, 1, 0, 1, 1, 0, 1, 2, 2, 0, 1, 2, 2, 0, 2, 0, 2, 2, 2, 1, 2, 1, 1, 1, 1, 0, 1, 1, 0, 1, 2]).all()\n",
    "assert (y_test_pred_pp == [1, 0, 2, 1, 2, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate prediction accuracy\n",
    "print('[+] For K-means:')\n",
    "train_score = np.sum(y_train_pred == y_train)\n",
    "print(f'Training accuracy: {train_score / len(y_train) * 100:.4f}% ({train_score}/{len(y_train)})')\n",
    "train_score = np.sum(y_test_pred == y_test)\n",
    "print(f'Training accuracy: {train_score / len(y_test) * 100:.4f}% ({train_score}/{len(y_test)})')\n",
    "print('[+] For K-means++:')\n",
    "train_score = np.sum(y_train_pred_pp == y_train)\n",
    "print(f'Training accuracy: {train_score / len(y_train) * 100:.4f}% ({train_score}/{len(y_train)})')\n",
    "train_score = np.sum(y_test_pred_pp == y_test)\n",
    "print(f'Training accuracy: {train_score / len(y_test) * 100:.4f}% ({train_score}/{len(y_test)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and plot out confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "fig = plt.subplot(2, 2, 1)\n",
    "sn.heatmap(confusion_matrix(y_train, y_train_pred), annot=True, cbar=False, square=True, linewidths=0.5)\n",
    "fig.set_title('K-means, train set')\n",
    "\n",
    "fig = plt.subplot(2, 2, 2)\n",
    "sn.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, cbar=False, square=True, linewidths=0.5)\n",
    "fig.set_title('K-means, test set')\n",
    "\n",
    "fig = plt.subplot(2, 2, 3)\n",
    "sn.heatmap(confusion_matrix(y_train, y_train_pred_pp), annot=True, cbar=False, square=True, linewidths=0.5)\n",
    "fig.set_title('K-means++, train set')\n",
    "\n",
    "fig = plt.subplot(2, 2, 4)\n",
    "sn.heatmap(confusion_matrix(y_test, y_test_pred_pp), annot=True, cbar=False, square=True, linewidths=0.5)\n",
    "fig.set_title('K-means++, test set');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}